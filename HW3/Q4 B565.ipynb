{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06db6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#corpus: 15 tweets using Omicron as the query (Jan 32, 2022)\n",
    "corpus = [\"DrCChambers RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19, CIHR is extending the registration and application de…\",\n",
    "\"sealsoftheend Japan's Kowa says Ivermectin showed 'antiviral effect' against Omicron https://t.co/mKKY24WeQV\",\n",
    "\"SVictor70973566 RT @EricTopol: Why is Omicron so hyper-transmissible? It's not related to high viral load in the upper airway, as shown by 2 recent studies…\",\n",
    "\"freethinkfacts RT @yaneerbaryam: Actual cases of reinfection by Omicron are so widespread they are manifest to anyone who is not closing their eyes: 10/…\",\n",
    "\"lsoril RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19, CIHR is extending the registration and application de…\",\n",
    "\"pompey1977 RT @AllisonPearson: How can people not get it? Omicron’s advantage over Delta is it evades the vaccine. Everyone is going to get Omicron…\",\n",
    "\"freethinkfacts RT @yaneerbaryam: Taken together, our results suggest that Omicron-induced immunity may not be sufficient to prevent infection from anothe…\",\n",
    "\"SteveBennett15 RT @EricTopol: Anyone who thinks that vaccines aren't working against Omicron might want to review the data https://t.co/9bHYdKxz8u https:/…\",\n",
    "\"wasohope RT @ASTERHealthcare: Omicron covid-19 variant was reported from South Africa on November 2021. This variant has had many mutations that aff…\",\n",
    "\"SVictor70973566 RT @MdFacep: @EricTopol @maxdkozlov Omicron's impact is in its ability to evade our immune system: Our 'older' vaccine produced NABS fail…\",\n",
    "\"Wildantlers @melulater Of course it did, Omicron spreads far quicker. But as a % of people who die from omicron it is far milde… https://t.co/eDFuW5qjAH\",\n",
    "\"peterandann RT @EnemyInAState: Omicron, London: Babies and toddlers continue to surge, and 1 in 9 admitted is a child. Over 668 babies and toddlers hav…\",\n",
    "\"DrMroz RT @CIHR_IRSC: In light of the disruptions caused by the Omicron variant of #Covid19, CIHR is extending the registration and application de…\",\n",
    "\"Deis85208721 CORRECTED-Japan's Kowa says ivermectin showed 'antiviral effect' against Omicron in research https://t.co/VEoQyz5x6F\",\n",
    "\"freethinkfacts RT @yaneerbaryam: Thus, breakthrough infection from Omicron may enhance cross-protection against Delta, and vice-versa, [only] inasmuch as…\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10b4dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words 22 ['antiviral effect' 'caused omicron' 'cihr extending' 'cihr_irsc light'\n",
      " 'covid19 cihr' 'disruptions caused' 'effect omicron'\n",
      " 'extending registration' 'freethinkfacts rt' 'ivermectin showed'\n",
      " 'japan kowa' 'kowa says' 'light disruptions' 'omicron variant'\n",
      " 'registration application' 'rt cihr_irsc' 'rt erictopol'\n",
      " 'rt yaneerbaryam' 'says ivermectin' 'showed antiviral'\n",
      " 'svictor70973566 rt' 'variant covid19']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 2), min_df=2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#print(\"stop words removed:\", vectorizer.stop_words_)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(f\"#words {len(words)} {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd50a6d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_array = X.toarray()\n",
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3593d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011010011001011100010111010111000100101110011001110101101000010\n",
      "0001101101011010011011001110110101101110010111011111100010000010\n",
      "0101111111011000100010110000010011000010110000010111100001000001\n",
      "1010100111010101000001101111010010110000001101010111111111010111\n",
      "1100010101000100110011001001000111110110100010111001010110011011\n",
      "1111101010001011010111001110100110110011100111000110100011001010\n",
      "0111011011001100111111110111111010110101001100100000111001110001\n",
      "0100011101101011110110011000110000001001010011111000110111001001\n",
      "1111010111101101011011001010101010001101100001010000001111101111\n",
      "0111111000101101011100111101010101101100101000001101111110001100\n",
      "1000010000101010001000110111110010000011001010111011011101110111\n",
      "1101000011100101101011001110100100011110110101000011100101111111\n",
      "1011001000000110010001111001011001010011111010001101110101010010\n",
      "0010101000011100010111011100110110011111101011010000001000000101\n",
      "1111111010100010010101110101001101000010010001111001111001100110\n",
      "1001101111000100110011000011100100110100101001000101110000011110\n",
      "0000001011101000100111000101100010101010001000111001101111001001\n",
      "1001110011111111000001010110011101111010110110110110000110001110\n",
      "1110000001011101110011011010101000000111001000111000011101101010\n",
      "1001011010111011101110111100101000000110000110100111110010110111\n",
      "1010100111100011000001010111111101100101111110011100010111000011\n",
      "0100011011000010110111000101101100111000110111101111000110111111\n"
     ]
    }
   ],
   "source": [
    "# Coverting features into hashed values but in binary format\n",
    "\n",
    "import hashlib\n",
    "\n",
    "hashedbin = list()\n",
    "for word in words:\n",
    "    word = word.encode('utf-8')\n",
    "    binword = bin(int.from_bytes(hashlib.sha256(word).digest(), 'little'))[-64:]\n",
    "    print(binword)\n",
    "    hashedbin.append([*binword])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c11e5",
   "metadata": {},
   "source": [
    "# SimHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23b3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
      "[1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "SIMHASH = [0]*len(X_array)\n",
    "for doc in range(len(X_array)):\n",
    "    document = X_array[doc]\n",
    "    hash = list()\n",
    "    for col in range(0,64):\n",
    "        addCol = 0\n",
    "        for row in range(len(hashedbin)):\n",
    "            binvalue = int(hashedbin[row][col])\n",
    "            if binvalue == 1:\n",
    "                addCol += binvalue*document[row]\n",
    "            else:\n",
    "                addCol -= 1*document[row]\n",
    "        if addCol < 0:\n",
    "            addCol = 0\n",
    "        else:\n",
    "            addCol = 1\n",
    "        hash.append(addCol)\n",
    "    SIMHASH[doc] = hash\n",
    "print(*SIMHASH,sep='\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3496e",
   "metadata": {},
   "source": [
    "# Calculating the similarity matrix for haming distance and euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1861de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix of Hamming distance\n",
      "[0, 39, 26, 30, 0, 29, 30, 29, 29, 23, 29, 29, 0, 39, 30]\n",
      "[39, 0, 31, 29, 39, 24, 29, 32, 24, 34, 24, 24, 39, 0, 29]\n",
      "[26, 31, 0, 22, 26, 15, 22, 21, 15, 13, 15, 15, 26, 31, 22]\n",
      "[30, 29, 22, 0, 30, 11, 0, 41, 11, 25, 11, 11, 30, 29, 0]\n",
      "[0, 39, 26, 30, 0, 29, 30, 29, 29, 23, 29, 29, 0, 39, 30]\n",
      "[29, 24, 15, 11, 29, 0, 11, 36, 0, 28, 0, 0, 29, 24, 11]\n",
      "[30, 29, 22, 0, 30, 11, 0, 41, 11, 25, 11, 11, 30, 29, 0]\n",
      "[29, 32, 21, 41, 29, 36, 41, 0, 36, 34, 36, 36, 29, 32, 41]\n",
      "[29, 24, 15, 11, 29, 0, 11, 36, 0, 28, 0, 0, 29, 24, 11]\n",
      "[23, 34, 13, 25, 23, 28, 25, 34, 28, 0, 28, 28, 23, 34, 25]\n",
      "[29, 24, 15, 11, 29, 0, 11, 36, 0, 28, 0, 0, 29, 24, 11]\n",
      "[29, 24, 15, 11, 29, 0, 11, 36, 0, 28, 0, 0, 29, 24, 11]\n",
      "[0, 39, 26, 30, 0, 29, 30, 29, 29, 23, 29, 29, 0, 39, 30]\n",
      "[39, 0, 31, 29, 39, 24, 29, 32, 24, 34, 24, 24, 39, 0, 29]\n",
      "[30, 29, 22, 0, 30, 11, 0, 41, 11, 25, 11, 11, 30, 29, 0]\n",
      "Similarity matrix of Euclidean distance\n",
      "[0.0, 4.24, 3.61, 3.61, 0.0, 3.32, 3.61, 3.46, 3.32, 3.46, 3.32, 3.32, 0.0, 4.24, 3.61]\n",
      "[4.24, 0.0, 3.0, 3.0, 4.24, 2.65, 3.0, 2.83, 2.65, 2.83, 2.65, 2.65, 4.24, 0.0, 3.0]\n",
      "[3.61, 3.0, 0.0, 2.0, 3.61, 1.41, 2.0, 1.0, 1.41, 1.0, 1.41, 1.41, 3.61, 3.0, 2.0]\n",
      "[3.61, 3.0, 2.0, 0.0, 3.61, 1.41, 0.0, 1.73, 1.41, 1.73, 1.41, 1.41, 3.61, 3.0, 0.0]\n",
      "[0.0, 4.24, 3.61, 3.61, 0.0, 3.32, 3.61, 3.46, 3.32, 3.46, 3.32, 3.32, 0.0, 4.24, 3.61]\n",
      "[3.32, 2.65, 1.41, 1.41, 3.32, 0.0, 1.41, 1.0, 0.0, 1.0, 0.0, 0.0, 3.32, 2.65, 1.41]\n",
      "[3.61, 3.0, 2.0, 0.0, 3.61, 1.41, 0.0, 1.73, 1.41, 1.73, 1.41, 1.41, 3.61, 3.0, 0.0]\n",
      "[3.46, 2.83, 1.0, 1.73, 3.46, 1.0, 1.73, 0.0, 1.0, 1.41, 1.0, 1.0, 3.46, 2.83, 1.73]\n",
      "[3.32, 2.65, 1.41, 1.41, 3.32, 0.0, 1.41, 1.0, 0.0, 1.0, 0.0, 0.0, 3.32, 2.65, 1.41]\n",
      "[3.46, 2.83, 1.0, 1.73, 3.46, 1.0, 1.73, 1.41, 1.0, 0.0, 1.0, 1.0, 3.46, 2.83, 1.73]\n",
      "[3.32, 2.65, 1.41, 1.41, 3.32, 0.0, 1.41, 1.0, 0.0, 1.0, 0.0, 0.0, 3.32, 2.65, 1.41]\n",
      "[3.32, 2.65, 1.41, 1.41, 3.32, 0.0, 1.41, 1.0, 0.0, 1.0, 0.0, 0.0, 3.32, 2.65, 1.41]\n",
      "[0.0, 4.24, 3.61, 3.61, 0.0, 3.32, 3.61, 3.46, 3.32, 3.46, 3.32, 3.32, 0.0, 4.24, 3.61]\n",
      "[4.24, 0.0, 3.0, 3.0, 4.24, 2.65, 3.0, 2.83, 2.65, 2.83, 2.65, 2.65, 4.24, 0.0, 3.0]\n",
      "[3.61, 3.0, 2.0, 0.0, 3.61, 1.41, 0.0, 1.73, 1.41, 1.73, 1.41, 1.41, 3.61, 3.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Hamming = list()\n",
    "Euclidean = list()\n",
    "for simHash1 in simHashes:\n",
    "    Hamming_Similarity = list()\n",
    "    Euclidean_Similarity = list()\n",
    "    for simHash2 in simHashes:\n",
    "        distance = 0\n",
    "        for col in range(len(simHash1)):\n",
    "            if simHash1[col] == simHash2[col]:\n",
    "                distance += 1\n",
    "        Hamming_Similarity.append(64-distance)\n",
    "        \n",
    "        point1 = X_array[simHashes.index(simHash1)]\n",
    "        point2 = X_array[simHashes.index(simHash2)]\n",
    "\n",
    "        Euclidean_Similarity.append(round(math.dist(point1, point2),2))\n",
    "\n",
    "    Euclidean.append(Euclidean_Similarity)\n",
    "    Hamming.append(Hamming_Similarity)\n",
    "print(\"Similarity matrix of Hamming distance\")\n",
    "print(*Hamming,sep='\\n')\n",
    "\n",
    "print(\"Similarity matrix of Euclidean distance\")\n",
    "print(*Euclidean,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda713c",
   "metadata": {},
   "source": [
    "# Computing the Pearson Correlation of the distances between all pairs of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9945837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pearson correlation between the distances for all pairs of documents = \n",
      " [[1.         0.79289978]\n",
      " [0.79289978 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "Hamming_array = np.array(Hamming).flatten()\n",
    "Euclidean_array = np.array(Euclidean).flatten()\n",
    "pearson_correlation = np.corrcoef(Hamming_array, Euclidean_array)\n",
    "\n",
    "print(\"The pearson correlation between the distances for all pairs of documents = \\n\", pearson_correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
