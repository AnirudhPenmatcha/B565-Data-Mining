{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_value(dataset):\n",
    "    datalist = []\n",
    "    for row in dataset:\n",
    "        temp_row = [column for column in row if not(pd.isnull(column)) == True]\n",
    "        datalist.append(temp_row)\n",
    "    return datalist\n",
    "\n",
    "\n",
    "def f1(dataset, min_support):\n",
    "    itemset = []\n",
    "    itemcount = {}\n",
    "    total_transaction = len(dataset)\n",
    "    unique_items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if item not in unique_items:\n",
    "                unique_items.add(item)\n",
    "                itemset.append([item])\n",
    "                itemcount[item] = 1\n",
    "            else:\n",
    "                itemcount[item] += 1\n",
    "    f1_set = []\n",
    "    for item, count in itemcount.items():\n",
    "        if count >= (min_support * len(dataset)):\n",
    "            f1_set.append([item])\n",
    "    return f1_set\n",
    "\n",
    "\n",
    "def merge_candidate(current_set, previous_set, size, flag):\n",
    "    pruned_set = []\n",
    "    if flag == 1:\n",
    "        for candidate in current_set:\n",
    "            isFrequent = True\n",
    "            for subset in combinations(candidate, size - 1):\n",
    "                if list(subset) not in previous_set:\n",
    "                    isFrequent = False\n",
    "                    break\n",
    "            if isFrequent:\n",
    "                pruned_set.append(candidate)\n",
    "    else:\n",
    "        pruned_set = copy.deepcopy(current_set)\n",
    "    return pruned_set\n",
    "\n",
    "def generate_candidates_f1_fk1(dataset, f1_set, min_support):\n",
    "    candidates = apriori(dataset, f1_set, min_support, 0)\n",
    "    return candidates\n",
    "\n",
    "def generate_candidates_fk1_fk1(dataset, f1_set, min_support):\n",
    "    candidates = apriori(dataset, f1_set, min_support, 1)\n",
    "    return candidates\n",
    "\n",
    "def apriori(dataset, candidate_set, min_support, flag):    \n",
    "    candidates = []\n",
    "    set_size = 1\n",
    "    f1 = copy.deepcopy(candidate_set)\n",
    "    count_candidate_gen = 0\n",
    "    \n",
    "    while candidate_set:\n",
    "        temp_candidates = []\n",
    "        pruned_temp_candidates = []\n",
    "        unique_set = set()\n",
    "        if flag == 1:\n",
    "            for i in range(len(candidate_set)):\n",
    "                items1 = candidate_set[i]\n",
    "                for j in range(i + 1, len(candidate_set)):\n",
    "                    items2 = candidate_set[j]\n",
    "                    if items1[:-1] == items2[:-1] and items1[-1] != items2[-1]:\n",
    "                        temp_items = copy.deepcopy(items1)\n",
    "                        temp_items.append(items2[-1])\n",
    "                        temp_items = sorted(temp_items)\n",
    "                        if tuple(temp_items) not in unique_set:\n",
    "                            unique_set.add(tuple(temp_items))\n",
    "                            temp_candidates.append(temp_items)\n",
    "            set_size += 1\n",
    "            pruned_temp_candidates = merge_candidate(temp_candidates, candidate_set, set_size, flag)\n",
    "        else:\n",
    "            for i in range(len(f1)):\n",
    "                items1 = f1[i]\n",
    "                for j in range(len(candidate_set)):\n",
    "                    items2 = candidate_set[j]\n",
    "                    if items1[-1] not in items2:\n",
    "                        temp_items = copy.deepcopy(items2) \n",
    "                        temp_items.append(items1[-1])\n",
    "                        temp_items = sorted(temp_items)\n",
    "                        if tuple(temp_items) not in unique_set:\n",
    "                            unique_set.add(tuple(temp_items))\n",
    "                            temp_candidates.append(temp_items)                            \n",
    "            set_size += 1\n",
    "            pruned_temp_candidates = merge_candidate(temp_candidates, candidate_set, set_size, flag)\n",
    "        \n",
    "        if len(pruned_temp_candidates) > 0:\n",
    "            count_candidate_gen = len(pruned_temp_candidates)\n",
    "            \n",
    "        itemcount = {}\n",
    "        for candidate in pruned_temp_candidates:\n",
    "             itemcount[tuple(candidate)] = 0\n",
    "                \n",
    "        for transaction in dataset:\n",
    "            for candidate in pruned_temp_candidates:\n",
    "                if set(candidate).issubset(transaction):\n",
    "                    itemcount[tuple(candidate)] += 1\n",
    "\n",
    "        new_itemset = []\n",
    "        for candidate, count in itemcount.items():\n",
    "            if count >= (min_support * len(dataset)):\n",
    "                new_itemset.append(list(candidate))\n",
    "        if len(new_itemset) > 0:\n",
    "            candidates = copy.deepcopy(new_itemset)\n",
    "        candidate_set = new_itemset\n",
    "        \n",
    "    return candidates, count_candidate_gen\n",
    "\n",
    "def rules_generator(frequent_set, min_confidence, dataset):\n",
    "    rules = []\n",
    "    for itemset in frequent_set:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1,len(itemset)):\n",
    "                for prefix in combinations(itemset, i):\n",
    "                    prefix = list(prefix)\n",
    "                    suffix = list(set(itemset) - set(prefix))\n",
    "                    \n",
    "                    itemset_support = 0\n",
    "                    for transaction in dataset:\n",
    "                        if set(itemset).issubset(transaction):\n",
    "                            itemset_support += 1\n",
    "                    \n",
    "                    prefix_support = 0\n",
    "                    for transaction in dataset:\n",
    "                        if set(prefix).issubset(transaction):\n",
    "                            prefix_support += 1\n",
    "                    \n",
    "                    confidence = itemset_support/prefix_support\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((prefix, suffix, confidence))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of candidates in f1 & fk-1 =  249\n",
      "The number of candidates in fk-1 & fk-1 =  3\n",
      "Candidate generation differences between the first and second =  246\n",
      "\n",
      "\n",
      "Frequent Itemsets f1 & fk-1 with a support >=  {0.04}\n",
      "['tropical fruit', 'whole milk']\n",
      "['whole milk', 'yogurt']\n",
      "['other vegetables', 'yogurt']\n",
      "['other vegetables', 'whole milk']\n",
      "['rolls/buns', 'whole milk']\n",
      "['soda', 'whole milk']\n",
      "['root vegetables', 'whole milk']\n",
      "['other vegetables', 'rolls/buns']\n",
      "['other vegetables', 'root vegetables']\n",
      "\n",
      "\n",
      "f1 & fk-1 association rules with a confidence >=  {0.07}\n",
      "['tropical fruit'] => ['whole milk'], Confidence: 0.40\n",
      "['whole milk'] => ['tropical fruit'], Confidence: 0.17\n",
      "['whole milk'] => ['yogurt'], Confidence: 0.22\n",
      "['yogurt'] => ['whole milk'], Confidence: 0.40\n",
      "['other vegetables'] => ['yogurt'], Confidence: 0.22\n",
      "['yogurt'] => ['other vegetables'], Confidence: 0.31\n",
      "['other vegetables'] => ['whole milk'], Confidence: 0.39\n",
      "['whole milk'] => ['other vegetables'], Confidence: 0.29\n",
      "['rolls/buns'] => ['whole milk'], Confidence: 0.31\n",
      "['whole milk'] => ['rolls/buns'], Confidence: 0.22\n",
      "['soda'] => ['whole milk'], Confidence: 0.23\n",
      "['whole milk'] => ['soda'], Confidence: 0.16\n",
      "['root vegetables'] => ['whole milk'], Confidence: 0.45\n",
      "['whole milk'] => ['root vegetables'], Confidence: 0.19\n",
      "['other vegetables'] => ['rolls/buns'], Confidence: 0.22\n",
      "['rolls/buns'] => ['other vegetables'], Confidence: 0.23\n",
      "['other vegetables'] => ['root vegetables'], Confidence: 0.24\n",
      "['root vegetables'] => ['other vegetables'], Confidence: 0.43\n",
      "\n",
      "\n",
      "The frequent itemsets from fk-1 & fk-1 with a support >=  {0.04}\n",
      "['tropical fruit', 'whole milk']\n",
      "['whole milk', 'yogurt']\n",
      "['other vegetables', 'yogurt']\n",
      "['other vegetables', 'whole milk']\n",
      "['rolls/buns', 'whole milk']\n",
      "['soda', 'whole milk']\n",
      "['root vegetables', 'whole milk']\n",
      "['other vegetables', 'rolls/buns']\n",
      "['other vegetables', 'root vegetables']\n",
      "\n",
      "\n",
      "fk-1 & fk-1 association rules with a confidence >=  {0.07}\n",
      "['tropical fruit'] => ['whole milk'], Confidence: 0.40\n",
      "['whole milk'] => ['tropical fruit'], Confidence: 0.17\n",
      "['whole milk'] => ['yogurt'], Confidence: 0.22\n",
      "['yogurt'] => ['whole milk'], Confidence: 0.40\n",
      "['other vegetables'] => ['yogurt'], Confidence: 0.22\n",
      "['yogurt'] => ['other vegetables'], Confidence: 0.31\n",
      "['other vegetables'] => ['whole milk'], Confidence: 0.39\n",
      "['whole milk'] => ['other vegetables'], Confidence: 0.29\n",
      "['rolls/buns'] => ['whole milk'], Confidence: 0.31\n",
      "['whole milk'] => ['rolls/buns'], Confidence: 0.22\n",
      "['soda'] => ['whole milk'], Confidence: 0.23\n",
      "['whole milk'] => ['soda'], Confidence: 0.16\n",
      "['root vegetables'] => ['whole milk'], Confidence: 0.45\n",
      "['whole milk'] => ['root vegetables'], Confidence: 0.19\n",
      "['other vegetables'] => ['rolls/buns'], Confidence: 0.22\n",
      "['rolls/buns'] => ['other vegetables'], Confidence: 0.23\n",
      "['other vegetables'] => ['root vegetables'], Confidence: 0.24\n",
      "['root vegetables'] => ['other vegetables'], Confidence: 0.43\n",
      "\n",
      " The potential solution for this was discussed with classmates\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('groceries.csv')\n",
    "# Cleaning the dataset\n",
    "# Remove the items column\n",
    "df.drop(columns='Item(s)', inplace=True)\n",
    "# Add all the values to a list\n",
    "df = df.values.tolist()\n",
    "# Call the function to remove all the empty values\n",
    "dataset = remove_empty_value(df)\n",
    "\n",
    "# Defining some default values\n",
    "min_support = 0.04\n",
    "min_confidence = 0.07\n",
    "f_1 = f1(dataset, min_support)\n",
    "candidates_f1_fk1, f1_fk1_candidate_gencount = generate_candidates_f1_fk1(dataset, f_1, min_support)\n",
    "candidates_fk1_fk1, fk1_fk1_candidate_gencount = generate_candidates_fk1_fk1(dataset, f_1, min_support)\n",
    "\n",
    "rules_f1_fk1 = rules_generator(candidates_f1_fk1, min_confidence, dataset)\n",
    "rules_fk1_fk1 = rules_generator(candidates_fk1_fk1, min_confidence, dataset)\n",
    "\n",
    "print(\"The number of candidates in f1 & fk-1 = \", f1_fk1_candidate_gencount)\n",
    "print(\"The number of candidates in fk-1 & fk-1 = \", fk1_fk1_candidate_gencount)\n",
    "print(\"Candidate generation differences between the first and second = \", abs(f1_fk1_candidate_gencount - fk1_fk1_candidate_gencount))\n",
    "print(\"\\n\")\n",
    "print(f\"Frequent Itemsets f1 & fk-1 with a support >= \", {min_support})\n",
    "for itemset in candidates_f1_fk1:\n",
    "    print(itemset)\n",
    "print(\"\\n\")\n",
    "print(\"f1 & fk-1 association rules with a confidence >= \", {min_confidence})\n",
    "for rule in rules_f1_fk1:\n",
    "    antecedent, consequent, confidence = rule\n",
    "    print(f\"{antecedent} => {consequent}, Confidence: {confidence:.2f}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The frequent itemsets from fk-1 & fk-1 with a support >= \", {min_support})\n",
    "for itemset in candidates_fk1_fk1:\n",
    "    print(itemset)\n",
    "print(\"\\n\")\n",
    "print(f\"fk-1 & fk-1 association rules with a confidence >= \", {min_confidence})\n",
    "for rule in rules_fk1_fk1:\n",
    "    antecedent, consequent, confidence = rule\n",
    "    print(f\"{antecedent} => {consequent}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "print(\"\\n The potential solution for this was discussed with classmates\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
